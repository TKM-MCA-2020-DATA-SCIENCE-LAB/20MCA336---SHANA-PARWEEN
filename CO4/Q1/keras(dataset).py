# -*- coding: utf-8 -*-
"""KERAS(DATASET).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YrUVgfG23H0pRSfisD4IGWTJhYDDCawk
"""

import keras
from keras.datasets import mnist 
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from matplotlib import pyplot as plt
import numpy as np
import seaborn as sb
import pandas as pd

df = pd.read_csv('/content/data.csv')
df

df.isnull().sum()

#droping feature
df.drop(['Unnamed: 32','id'],axis=1,inplace=True)

df

# independent variables
x = df.drop('diagnosis',axis=1)
#dependent variables
y = df.diagnosis

x

y

from sklearn.preprocessing import LabelEncoder
#creating the object
lb = LabelEncoder()
y = lb.fit_transform(y)

y

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=40) #splitting data

x_train.shape

y_train.shape

type(x_train)

#importing StandardScaler
from sklearn.preprocessing import StandardScaler
#creating object
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

#importing keras
import keras
#importing sequential module
from keras.models import Sequential
# import dense module for hidden layers
from keras.layers import Dense
#importing activation functions
from keras.layers import LeakyReLU,PReLU,ELU
from keras.layers import Dropout

x_train[0]

x_test[0]

#creating model
model = Sequential()

#first hidden layer
model.add(Dense(units=9,kernel_initializer='he_uniform',activation='relu',input_dim=30))
#second hidden layer
model.add(Dense(units=9,kernel_initializer='he_uniform',activation='relu'))
# last layer or output layer
model.add(Dense(units=1,kernel_initializer='glorot_uniform',activation='sigmoid'))

#taking summary of layers
model.summary()

#compiling the ANN
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

#fitting the ANN to the training set
model = model.fit(x_train,y_train,batch_size=100,epochs=100)

# list all data in history
print(model.history.keys())
# summarize history for accuracy
plt.plot(model.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(model.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()